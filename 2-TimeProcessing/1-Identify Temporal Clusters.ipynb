{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2, multiprocessing, psycopg2.extras, os, json, sys, time, scipy, datetime\n",
    "from multiprocessing import Pool, Manager \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape, mapping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_directory  = \"/data/chime/geo/zone_a_full_contexual/stage1\"\n",
    "output_directory = \"/data/chime/geo/zone_a_full_contexual/stage2\"\n",
    "\n",
    "#SANDY\n",
    "_landfall_str = '201210300000' #Need to define the new dates for Hurricane Matthew in these zones?\n",
    "_start_str    = '201210290000' # (ie when was the evacuation?)\n",
    "_end_str      = '201210310000'\n",
    "_landfall = pd.Timestamp(_landfall_str)\n",
    "_start    = pd.Timestamp(_start_str)\n",
    "_end      = pd.Timestamp(_end_str)\n",
    "\n",
    "#MATTHEW\n",
    "# _landfall_str = '201610060000' #Using these dates for Matthew...\n",
    "# _start_str    = '201610040000' # (ie when was the evacuation?)\n",
    "# _end_str      = '201610080000'\n",
    "# _landfall = pd.Timestamp(_landfall_str)\n",
    "# _start    = pd.Timestamp(_start_str)\n",
    "# _end      = pd.Timestamp(_end_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load all the geo-clustered tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 users in /data/chime/geo/zone_a_full_contexual/stage1\n"
     ]
    }
   ],
   "source": [
    "users_in = sorted(os.listdir(input_directory))\n",
    "print(\"Found {0} users in {1}\".format(len(users_in), input_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loader_function(args):\n",
    "    uFile, path, q = args\n",
    "    u = json.load(open(path+\"/\"+uFile,'r'))\n",
    "    tweets = []\n",
    "    for t in u['features']:\n",
    "        if t['geometry']:\n",
    "            t['properties']['geometry'] = shape(t['geometry'])\n",
    "        t['properties']['date'] = pd.Timestamp(t['properties']['date'])\n",
    "        tweets.append(t['properties'])\n",
    "    q.put(1)\n",
    "    return gpd.GeoDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 96, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, input_directory, q) for i in users_in]\n",
    "result = p.map_async(loader_function, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))\n",
    "users = result.get()\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107828 total tweets for all users\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} total tweets for all users\".format(sum([len(x) for x in users])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Identifying Temporal Clusters\n",
    "Use a custom _worker_ function to find specific time clusters\n",
    "\n",
    "## 1. Enough Tweets?\n",
    "\n",
    "Ensure that we have the following for each user:\n",
    "1. Geo-Cluter Information (If no geo-clusters are available, remove)\n",
    "2. Enough Tweets (At least *A* Tweet during the storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_cluster(t):\n",
    "    t = t.tz_convert(\"EST\")\n",
    "    '''Get the timecluster'''\n",
    "    hour = t.hour//4 + 1\n",
    "    if t.weekday()>4:\n",
    "        return 6+hour\n",
    "    else:\n",
    "        return hour\n",
    "    \n",
    "def worker_function(args):\n",
    "    userDF, q = args\n",
    "    \n",
    "    #If no tweets around the time of the storm, then fail.\n",
    "    if len(userDF.query(\"date > %s & date < %s\"%(_start_str, _end_str))) < 1:\n",
    "        q.put(1)\n",
    "        return None\n",
    "    \n",
    "    userDF['day_cluster'] = userDF.date.apply(lambda t: time_cluster(t))\n",
    "    q.put(0)\n",
    "    return userDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 96, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, q) for i in users]\n",
    "result = p.map_async(worker_function, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))\n",
    "\n",
    "values = result.get()\n",
    "x = [i for i in values  if i is not None]\n",
    "nones = [i for i in values  if i is None]\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 96 users\n",
      "0 Users failed\n"
     ]
    }
   ],
   "source": [
    "print(\"Successfully processed {0} users\\n{1} Users failed\".format(len(x),len(nones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_users = sorted(x, key=lambda y: len(y), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Group the clusters and find which geo-clusters correspond with home hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_clusters(df):\n",
    "    \"\"\"\n",
    "    There is definitely room for the logic in _this_ function to improve, but for now it looks good :) \n",
    "    \"\"\"\n",
    "    gb_geo = df.query('date < '+_start_str).groupby('cluster')\n",
    "    if len(gb_geo) < 1:\n",
    "        return (None,None)\n",
    "    _agged = gb_geo['day_cluster'].agg({\"tweets\":pd.Series.count,\n",
    "                                        \"Number Unique Times\":pd.Series.nunique,\n",
    "                                        \"day_cluster_counts\": lambda t: Counter(t),\n",
    "                                        \"HomeTimes\": lambda t: any(t==1) or any(t==2) or any(t==6),\n",
    "                                      }).sort_values('Number Unique Times', ascending=False)\n",
    "    hc = None\n",
    "#   If the highest rated cluster (unique timewise includes HomeTimes, then return that)\n",
    "    if(_agged.HomeTimes.values[0]):\n",
    "        hc = _agged.iloc[0].name\n",
    "    return hc, _agged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96 of 96\n",
      "\n",
      "Clustered: 81, Failed: 15"
     ]
    }
   ],
   "source": [
    "vals= []\n",
    "len_users = len(_users)\n",
    "user_collection = []\n",
    "\n",
    "user_meta_collection = []\n",
    "for idx, U in enumerate(_users):\n",
    "    \n",
    "    hc = rank_clusters(U)[0]\n",
    "    if hc==None or hc < 0:\n",
    "        hc = None\n",
    "        hc_coords = None\n",
    "    else:\n",
    "        hc_coords = U.query(\"cluster=={0}\".format(hc))['cluster_center'].values[0]\n",
    "        \n",
    "    user_meta_collection.append({\n",
    "            'user':U['user'].values[0],\n",
    "            'uid' :U['uid'].values[0],\n",
    "            'tweets':len(U),\n",
    "            'home_cluster': hc,\n",
    "            'home_cluster_coords':hc_coords\n",
    "        })\n",
    "\n",
    "    U['home_cluster_id'] = pd.Series(hc)\n",
    "    \n",
    "    sys.stderr.write(\"\\r{0} of {1}\".format(idx+1, len_users))\n",
    "_user_meta = pd.DataFrame(user_meta_collection)\n",
    "sys.stderr.write(\"\\n\\nClustered: {0}, Failed: {1}\".format( len(_user_meta[~np.isnan(_user_meta.home_cluster)]),\n",
    "                                                           len(_user_meta[np.isnan(_user_meta.home_cluster)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_with_homes = [u for u in _users if u.home_cluster_id.count()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export the User Meta Dataframe first\n",
    "with open(output_directory+'/'+'temporal_clustered_user_meta.json','w') as metaOut:\n",
    "    metaOut.write(_user_meta.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_cluster</th>\n",
       "      <th>home_cluster_coords</th>\n",
       "      <th>tweets</th>\n",
       "      <th>uid</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5007</td>\n",
       "      <td>10999012</td>\n",
       "      <td>MadMike4883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>4880</td>\n",
       "      <td>33667217</td>\n",
       "      <td>payalll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>4566</td>\n",
       "      <td>177665053</td>\n",
       "      <td>mutalib_ahmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"type\": \"Point\", \"coordinates\": [-73.92728007...</td>\n",
       "      <td>3990</td>\n",
       "      <td>523156158</td>\n",
       "      <td>LoveM_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"type\": \"Point\", \"coordinates\": [-74.10479844...</td>\n",
       "      <td>3882</td>\n",
       "      <td>21446190</td>\n",
       "      <td>shannonnielsenn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   home_cluster                                home_cluster_coords  tweets  \\\n",
       "0           NaN                                               None    5007   \n",
       "1           NaN                                               None    4880   \n",
       "2           NaN                                               None    4566   \n",
       "3           1.0  {\"type\": \"Point\", \"coordinates\": [-73.92728007...    3990   \n",
       "4           1.0  {\"type\": \"Point\", \"coordinates\": [-74.10479844...    3882   \n",
       "\n",
       "         uid             user  \n",
       "0   10999012      MadMike4883  \n",
       "1   33667217          payalll  \n",
       "2  177665053    mutalib_ahmed  \n",
       "3  523156158         LoveM_11  \n",
       "4   21446190  shannonnielsenn  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_user_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Write these users to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_mapping(p):\n",
    "    if p==None or np.isnan(p).any():\n",
    "        return None\n",
    "    else:\n",
    "        return mapping(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_json_export(args):\n",
    "    df, path, q = args\n",
    "    df = df.copy()\n",
    "    uName = df.head(1).user.values[0].lower()\n",
    "    df['date'] = df['date'].apply(lambda t: datetime.datetime.strftime(t,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "    #Write the metadata:\n",
    "    with open('/data/www/chime/movement-derivation/user-metadata/'+uName+\".geojson\",'w') as userMeta:\n",
    "        userMeta.write(df.query(\"cluster=={0}\".format(df.iloc[0].home_cluster_id)).cluster_center.values[0])\n",
    "\n",
    "    clean = df.where((pd.notnull(df)), None)\n",
    "    geojson = {\"type\":\"FeatureCollection\",\"features\":[]}\n",
    "    for _, row in clean.iterrows():\n",
    "        geom = safe_mapping(row.geometry)\n",
    "        feature = {'type':'Feature',\n",
    "                   'geometry':geom,\n",
    "                   'properties':row.to_dict()\n",
    "                    }\n",
    "        del feature['properties']['geometry']\n",
    "        geojson['features'].append(feature)\n",
    "    \n",
    "    with open(path+\"/\"+uName+'.geojson','w') as oFile:\n",
    "        json.dump(geojson, oFile) \n",
    "\n",
    "    if q is not None:\n",
    "        q.put(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe_json_export((users_with_homes[0],output_directory,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting 81 users to /data/chime/geo/zone_a_full_contexual/stage2\n",
      "Processed: 81, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, output_directory, q) for i in users_with_homes]\n",
    "result = p.map_async(safe_json_export, args)\n",
    "\n",
    "sys.stderr.write(\"Exporting {0} users to {1}\\n\".format(len(args),output_directory))\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "p.close()\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "# End of Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "# Beginning of Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
