{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoProcessing:\n",
    "\n",
    "## DBScan: Cluster Analysis of every user's location\n",
    "## Speed Processing (Check if a user's reported speeds are even possible!)\n",
    "\n",
    "Using DBScan, find spatial clusters relevant to each user, typically home, work, or school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json, matplotlib, iso8601, sys, time, datetime, pytz\n",
    "from shapely.geometry import shape\n",
    "from multiprocessing import Pool, Manager;\n",
    "import numpy as np; import pandas as pd; import geopandas as gpd\n",
    "from dbscan_python import dbscan\n",
    "from geodistance import geodistance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21951"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_with_tweets_in_zone_a = sorted(os.listdir('../working_data/all_tweets_from_zoneA_users/'))\n",
    "len(users_with_tweets_in_zone_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: User .geojsonl file\n",
    "    Returns: DataFrame with speed & clusters\n",
    "\n",
    "    #Process\n",
    "    1. Loads all of a user's tweets (parseable json per line, parse, strip down to details, sort by time)\n",
    "    2. Computes Clusters based on below EPS & MIN_PTS\n",
    "    3. Computes time, distance, and speed between each tweet\n",
    "    4. Puts it all into a DataFrame\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def worker_function(args):\n",
    "\n",
    "    user_geojsonl_file, q = args\n",
    "\n",
    "    EPS     = 100     #Max. Distance for points in the cluster... (In meters)                                                                                                                  \n",
    "    MIN_PTS = 3       #Minimum Points in a cluster...\n",
    "    tweets = []\n",
    "    for line in open('../working_data/all_tweets_from_zoneA_users/'+user_geojsonl_file,'r'):\n",
    "        t = json.loads(line.strip())\n",
    "        stripped = {\n",
    "                'geometry': shape(t['geometry']),\n",
    "                'coords'  : t['geometry']['coordinates'],\n",
    "                'date'    : iso8601.parse_date(t['properties']['postedTime']),\n",
    "                'text'    : t['properties']['body'],\n",
    "                'user'    : t['properties']['actor']['preferredUsername']\n",
    "            }\n",
    "        tweets.append(stripped)\n",
    "    if len(tweets)==1:\n",
    "        q.put(1)\n",
    "        return None\n",
    "    #Ensure we're sorted by time:\n",
    "    tweets.sort(key=lambda t: t['date'])\n",
    "\n",
    "    #Clustering:\n",
    "    points = [t['coords'] for t in tweets]\n",
    "    m = np.matrix([ [p[0] for p in points], [p[1] for p in points] ])\n",
    "#     print(m)\n",
    "    clusters = dbscan.dbscan(m, EPS, MIN_PTS)\n",
    "    \n",
    "    #Iterate through, assign clusters & calculate distance\n",
    "    for idx, t in enumerate(tweets):\n",
    "        t['cluster'] = clusters[idx]\n",
    "        if idx>0:\n",
    "            p1 = t['geometry']\n",
    "            p2 = tweets[idx-1]['geometry']\n",
    "            t['geo_delta'] = geodistance.distanceHaversine(p1.y,p1.x,p2.y,p2.x)[0]*1000\n",
    "    \n",
    "    #Now we can finally do the dataframe bullshit...\n",
    "    df = gpd.GeoDataFrame(tweets)\n",
    "\n",
    "    # If clusters were all None, then return nothing, it couldn't cluster!\n",
    "    if len(df.cluster.value_counts()) < 1:\n",
    "        q.put(2)\n",
    "        return None\n",
    "    df['time_delta'] = df['date'].diff()\n",
    "    df['speed'] = df.apply(lambda row: row['geo_delta']/ (row['time_delta'] / np.timedelta64(1, 's')), axis=1)\n",
    "    q.put(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 3431, 15.6%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, q) for i in users_with_tweets_in_zone_a]\n",
    "result = p.map_async(worker_function, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        with open(\"load.log\",'w') as log:\n",
    "            log.write(\"\\rProcessed: {0}, {1:.3g}%\\n\".format(size, size/len(args)*100))\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(2)\n",
    "\n",
    "values = result.get()\n",
    "users = [i for i in values if i is not None]\n",
    "nones = [i for i in values if i is None]\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check that the GeoDataFrame is working...\n",
    "users[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.mkdir('../working_data/clustered_three_pts_with_speed_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_json_export(args):\n",
    "    path = '../working_data/clustered_three_pts_with_speed_2/'\n",
    "    df, q = args\n",
    "    df = df.copy()\n",
    "    df['date'] = df['date'].apply(lambda t: datetime.datetime.strftime(t,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "    df['time_delta'] = df['time_delta'] / np.timedelta64(1, 's')\n",
    "    uName = df.head(1).user.values[0].lower()\n",
    "    with open(path+uName+'.geojson','w') as oFile:\n",
    "        oFile.write(df.to_json(ensure_ascii=False))\n",
    "    q.put(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, q) for i in users]\n",
    "result = p.map_async(safe_json_export, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Scrap Below... \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Will Move this to temporal processing after we can serialize these dataframes better (DONE) :) \n",
    "def check_before_and_after(df):\n",
    "    _before   = datetime.datetime(2012,10,29,8,0,0,tzinfo=pytz.UTC) #Noon\n",
    "    _landfall = datetime.datetime(2012,10,30,0,0,0,tzinfo=pytz.UTC) #8pm EST\n",
    "    _after    = datetime.datetime(2012,10,31,0,0,0,tzinfo=pytz.UTC) #8pm EST\n",
    "    \n",
    "    tweets_during_landfall_p1 = df[df['date'] > _before]\n",
    "    tweets_during_landfall    = tweets_during_landfall_p1[tweets_during_landfall_p1['date'] < _after]\n",
    "    \n",
    "    if len(tweets_during_landfall) > 1:\n",
    "        return tweets_during_landfall\n",
    "    else:\n",
    "        return None\n",
    "check_before_and_after(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atLandfall = [check_before_and_after(x) for x in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atLandfallReal = [x for x in atLandfall if type(x) != type(None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "landfallers = sorted(atLandfallReal, key=lambda x: len(x), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
