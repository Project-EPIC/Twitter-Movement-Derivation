{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, iso8601, fiona, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Clean, Sort, Export from GNIP\n",
    "\n",
    "This notebook: \n",
    "\n",
    "1. Loads raw gnip data\n",
    "1. Filters for a geotag\n",
    "1. Identifies all tweets within an area of interest\n",
    "1. Finds Unique Users\n",
    "1. Writes full GNIP GEOJSONL files per user for users with at least 3 tweets in the area of interest\n",
    "\n",
    "## [Click here for Spark Status](http://epic-analytics.cs.colorado.edu:4040/jobs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the files\n",
    "raw_strings = sc.textFile('/data/chime/matthew/gnip-geo/ws*/*')\n",
    "\n",
    "#Filter out the duds\n",
    "strings = raw_strings.filter(lambda x: x!=\"\")\n",
    "\n",
    "#JSONs\n",
    "jsons  = strings.map(json.loads)\n",
    "\n",
    "tweet_jsons = jsons.filter(lambda x: 'info' not in x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load all the Tweets!\n",
    "### 2 Crucial Steps\n",
    "1. A lot of tweets do not actually have lat/lon that will work for our purposes\n",
    "1. The GNIP `geo` field is backwards from convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_geo(t):\n",
    "    t['geo']['coordinates'].reverse()\n",
    "    return t\n",
    "\n",
    "tweets_with_geo = tweet_jsons.filter(lambda t: 'geo' in t.keys())\n",
    "\n",
    "geo_tweets = tweets_with_geo.map(fix_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that this is working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'actor': {u'displayName': u'DJDADDY',\n",
      "            u'favoritesCount': 6131,\n",
      "            u'followersCount': 1016,\n",
      "            u'friendsCount': 350,\n",
      "            u'id': u'id:twitter.com:1097299662',\n",
      "            u'image': u'https://pbs.twimg.com/profile_images/733006233481928704/w7EXSmBe_normal.jpg',\n",
      "            u'languages': [u'en'],\n",
      "            u'link': u'http://www.twitter.com/DJDADDY_PRO',\n",
      "            u'links': [{u'href': u'http://www.djdaddypro.com',\n",
      "                        u'rel': u'me'}],\n",
      "            u'listedCount': 11,\n",
      "            u'location': {u'displayName': u'Hollywood, FL',\n",
      "                          u'objectType': u'place'},\n",
      "            u'objectType': u'person',\n",
      "            u'postedTime': u'2013-01-17T05:47:27.000Z',\n",
      "            u'preferredUsername': u'DJDADDY_PRO',\n",
      "            u'statusesCount': 5875,\n",
      "            u'summary': u'Calienta El Party Con Dj Daddy\\u270c\\ufe0f',\n",
      "            u'twitterTimeZone': u'Atlantic Time (Canada)',\n",
      "            u'utcOffset': u'-10800',\n",
      "            u'verified': False},\n",
      " u'body': u'Lunes Pal Que Pueda , Botellas $80 La Noche Entrada Completamente Gratis , Las Damas Toman\\u2026 https://t.co/HXSyg7djbt',\n",
      " u'favoritesCount': 0,\n",
      " u'generator': {u'displayName': u'Instagram',\n",
      "                u'link': u'http://instagram.com'},\n",
      " u'geo': {u'coordinates': [-80.23691, 25.80893], u'type': u'Point'},\n",
      " u'gnip': {u'matching_rules': [{u'id': 8992136771280536924,\n",
      "                                u'tag': u'id:578,radii:34,center:[-80.26659987668491,25.852441951180566]'}],\n",
      "           u'urls': [{u'expanded_status': 200,\n",
      "                      u'expanded_url': u'https://www.instagram.com/p/BJlpWHOhA2U/',\n",
      "                      u'expanded_url_description': u'See this Instagram photo by @djdaddy_pro \\u2022 209 likes',\n",
      "                      u'expanded_url_title': u'Instagram photo by Dj Daddy Pro \\u2022 Aug 26, 2016 at 10:11pm UTC',\n",
      "                      u'url': u'https://t.co/HXSyg7djbt'}]},\n",
      " u'id': u'tag:search.twitter.com,2005:769399207627218944',\n",
      " u'link': u'http://twitter.com/DJDADDY_PRO/statuses/769399207627218944',\n",
      " u'location': {u'country_code': u'United States',\n",
      "               u'displayName': u'Miami, FL',\n",
      "               u'geo': {u'coordinates': [[[-80.321683, 25.70904],\n",
      "                                          [-80.321683, 25.855667],\n",
      "                                          [-80.144974, 25.855667],\n",
      "                                          [-80.144974, 25.70904]]],\n",
      "                        u'type': u'Polygon'},\n",
      "               u'link': u'https://api.twitter.com/1.1/geo/id/04cb31bae3b3af93.json',\n",
      "               u'name': u'Miami',\n",
      "               u'objectType': u'place',\n",
      "               u'twitter_country_code': u'US',\n",
      "               u'twitter_place_type': u'city'},\n",
      " u'object': {u'id': u'object:search.twitter.com,2005:769399207627218944',\n",
      "             u'link': u'http://twitter.com/DJDADDY_PRO/statuses/769399207627218944',\n",
      "             u'objectType': u'note',\n",
      "             u'postedTime': u'2016-08-27T05:00:40.000Z',\n",
      "             u'summary': u'Lunes Pal Que Pueda , Botellas $80 La Noche Entrada Completamente Gratis , Las Damas Toman\\u2026 https://t.co/HXSyg7djbt'},\n",
      " u'objectType': u'activity',\n",
      " u'postedTime': u'2016-08-27T05:00:40.000Z',\n",
      " u'provider': {u'displayName': u'Twitter',\n",
      "               u'link': u'http://www.twitter.com',\n",
      "               u'objectType': u'service'},\n",
      " u'retweetCount': 0,\n",
      " u'twitter_entities': {u'hashtags': [],\n",
      "                       u'symbols': [],\n",
      "                       u'urls': [{u'display_url': u'instagram.com/p/BJlpWHOhA2U/',\n",
      "                                  u'expanded_url': u'https://www.instagram.com/p/BJlpWHOhA2U/',\n",
      "                                  u'indices': [92, 115],\n",
      "                                  u'url': u'https://t.co/HXSyg7djbt'}],\n",
      "                       u'user_mentions': []},\n",
      " u'twitter_filter_level': u'low',\n",
      " u'twitter_lang': u'es',\n",
      " u'verb': u'post'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(geo_tweets.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Export subsets of the data as `.geojsonl` files\n",
    "This is not by user, this is all tweets in one file (i.e., BIG)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "all_geo_tweets = tweets_with_geo.collect()\n",
    "write_full_tweets_to_geojsonl('all_geotagged_tweets_full', all_geo_tweets)\n",
    "write_bare_tweets_to_geojsonl('all_geotagged_tweets_bare', all_geo_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Group tweets by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_gb_user = geo_tweets.groupBy(lambda t: t['actor']['id'])\n",
    "tweets_gb_user.cache() # Cache here because we'll use this over and over and over....\n",
    "tweets_gb_user.count() # If you don't call this, it won't actually cache anything :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on the status of this operation, should see a tuple of: `(user_id, iterable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'id:twitter.com:45516704',\n",
      " <pyspark.resultiterable.ResultIterable object at 0x7f01db6a0890>)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tweets_gb_user.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_simplified_geo_contextual(tuple_of_uid_tweets):\n",
    "    u_tweets = list(tuple_of_uid_tweets[1])\n",
    "    u_tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "    fileName = u_tweets[0]['actor']['preferredUsername'].lower()\n",
    "    write_simplified_tweets_to_geojsonl('../working_data/simplified_geo_contextual_all_users/'+fileName,u_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Write the `geo-tweet-streams` for EVERY user "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tweets_gb_user.foreach( write_out_simplified_geo_contextual )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Geographic Filtering\n",
    "\n",
    "Load the shapefile for Evacuation Zone (or whatever your bounds should be)\n",
    "\n",
    "Currently this is primed for Hurricane Sandy, but for Matthew, just add the FL Evac Zone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import mapping, shape\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOVULNERABLE_AREAS = [\n",
    "    {\"name\": \"FL_Brevard_ZoneA\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/brevard_zone_a\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/brevard_zone_a_hull.geojson\"},\n",
    "    {\"name\": \"FL_DuvallCounty\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/duvall_county\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/DuvallCountyPlus_Hull.geojson\"},\n",
    "    {\"name\": \"FL_Indian_Martin_Lucie_Counties\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/indian_martin_lucie_counties\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/martin_indian_lucie_hull.geojson\"},\n",
    "    {\"name\": \"FL_Martin_Palm_Beach_Counties\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/martin_palm_beach_counties\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/martin_palm_beach_hull.geojson\"},\n",
    "    {\"name\": \"FL_Martin_Palm_Beach_Counties_Inland\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/martin_palm_beach_counties_inland\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/martin_palm_beach_inland_hull.geojson\"},\n",
    "    {\"name\": \"FL_Volusia_County\",           \n",
    "     \"outDir\": \"/data/chime/geo2/FL/volusia_county\",          \n",
    "     \"inFile\": \"../EvacuationZones/Florida/VolusiaPlus_Hull.geojson\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Only for SHAPE FILES\n",
    "\n",
    "for zone in GEOVULNERABLE_AREAS:\n",
    "    print(zone['name'])\n",
    "    c = fiona.open(zone['inFile'],'r')\n",
    "    pol = c.next()\n",
    "    zone['geom'] = shape(pol['geometry'])\n",
    "    with open(\"/data/www/jennings/geovulnerable_geoms/\"+zone['name']+\".geojson\",'w') as o:\n",
    "        json.dump(mapping(zone['geom']),o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_Brevard_ZoneA\n",
      "0.176479218337\n",
      "True\n",
      "FL_DuvallCounty\n",
      "0.543849526144\n",
      "True\n",
      "FL_Indian_Martin_Lucie_Counties\n",
      "0.102402136596\n",
      "True\n",
      "FL_Martin_Palm_Beach_Counties\n",
      "0.0590595896808\n",
      "True\n",
      "FL_Martin_Palm_Beach_Counties_Inland\n",
      "0.145108061803\n",
      "True\n",
      "FL_Volusia_County\n",
      "0.115224000409\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## For GeoJSON Files\n",
    "\n",
    "for zone in GEOVULNERABLE_AREAS:\n",
    "    print(zone['name'])\n",
    "    zone_geojson = json.load(open(zone['inFile'],'r'));\n",
    "    zone['geom'] = shape(zone_geojson['geometry'])\n",
    "    print(zone['geom'].area)\n",
    "    print(zone['geom'].is_valid)\n",
    "    with open(\"/data/www/jennings/geovulnerable_geoms/\"+zone['name']+\".geojson\",'w') as o:\n",
    "        json.dump(mapping(zone['geom']),o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zonesBroadcast = sc.broadcast(GEOVULNERABLE_AREAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over each of the grouped by users and determine which zones they may or may not belong in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkZoneWriteUser(points, tweets, zone, outDir):\n",
    "    \"\"\"\n",
    "    Given a list of points, tweets, the zone, and an output directory, check if at least 3 tweets land in the zone.\n",
    "    If so, write the user to disk and return.\n",
    "    \"\"\"\n",
    "    inBounds = 0;\n",
    "    for p in points:\n",
    "        if zone.contains(p):\n",
    "            inBounds += 1\n",
    "            if inBounds > 2:\n",
    "                write_full_tweets_to_geojsonl(outDir+'/'+tweets[0]['actor']['preferredUsername'].lower(),tweets)\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processZone(userIterable):\n",
    "    #Iterate over all of a user's tweets\n",
    "    zoneCounter = [{\"geom\":z['geom'], \"dir\":z['outDir']} for z in zonesBroadcast.value]\n",
    "    t_points = [shape(t['geo']) for t in userIterable[1]]\n",
    "    u_tweets = list(userIterable[1])\n",
    "    fileName = u_tweets[0]['actor']['preferredUsername'].lower()\n",
    "\n",
    "    for zone in zoneCounter:\n",
    "        checkZoneWriteUser(t_points, u_tweets, zone['geom'], zone['dir'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = tweets_gb_user.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boom! Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_gb_user.foreach(processZone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Export Functions (Requires Local Arrays)\n",
    "\n",
    "> Be sure to run this section to enable export functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_full_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        for t in tweets:\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':t['geo'],\n",
    "                'properties':t #Full GNIP Tweet in the properties (BIG)\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_simplified_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open('../working_data/'+fileName+'.geojsonl','w') as outFile:\n",
    "        for t in tweets:\n",
    "            if 'location' in t:\n",
    "                loc = t['location']\n",
    "            else:\n",
    "                loc = None\n",
    "            if 'location' in t['actor']:\n",
    "                u_loc = t['actor']['location']\n",
    "            else:\n",
    "                u_loc = None\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':t['geo'],\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'uid' :t['actor']['id'],\n",
    "                    'u_loc':u_loc,\n",
    "                    'u_reg':t['actor']['postedTime'],\n",
    "                    'u_sum':t['actor']['summary'],\n",
    "                    'tid' :t['id'],\n",
    "                    'loc' :loc,\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body'],\n",
    "                    'source':t['generator'],\n",
    "                    'verb':t['verb'],\n",
    "                    'meta':t['twitter_entities'],\n",
    "                    'u_utc':t['actor']['utcOffset']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_bare_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open('../working_data/'+fileName+'.geojsonl','w') as outFile:\n",
    "        for t in tweets:\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':t['geo'],\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set as a broadcast variable for spark\n",
    "zonesBroadcast = sc.broadcast(GEOVULNERABLE_AREAS)\n",
    "# #Testing (For ZoneA):\n",
    "# in_zone_a = shape({\"type\": \"Point\", \"coordinates\": [-73.75336647033691,40.599095409829815]})\n",
    "# in_bounds = shape({'type': \"Point\", 'coordinates': [-73.99154663085938,40.361195540839]})\n",
    "# print in_bounds\n",
    "# print \"TRUE?\", zoneBroadcast.value.contains(in_zone_a)\n",
    "# print \"TRUE?\", in_zone_a.within(zoneBroadcast.value)\n",
    "\n",
    "# out_of_bounds = shape({\"type\": \"Point\",\"coordinates\": [-73.99753114562988,40.73093368341445]})\n",
    "# print out_of_bounds\n",
    "# print \"FALSE?\", zoneBroadcast.value.contains(out_of_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter for those tweets in ZoneA\n",
    "inZoneA = tweets_with_geo.filter(lambda t: zoneABroadcast.value.contains( shape(t['geo']) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Group by user (for inside of Zone A)\n",
    "inZoneA_gb_user = inZoneA.groupBy(lambda t: t['actor']['id']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_with_at_least_one_tweet_in_zoneA = [u[0] for u in inZoneA_gb_user]\n",
    "len(users_with_at_least_one_tweet_in_zoneA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# json.dump(users_with_at_least_one_tweet_in_zoneA, open('../working_data/users_with_at_least_one_tweet_in_zoneA.json','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the ZoneA tweets, but first, ensure it's sorted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tweets[0]['postedTime'])\n",
    "iso8601.parse_date(tweets[0]['postedTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (user, tweets) in inZoneA_gb_user:\n",
    "    tweets = [t for t in tweets]\n",
    "    userName = tweets[0]['actor']['preferredUsername'].lower()\n",
    "    write_full_tweets_to_geojsonl('../working_data/tweets_in_zone_a_by_user/'+userName, tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Tweets from these Users (Beyond just those in ZoneA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21951"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_with_one_tweet_in_zoneA = json.load(open('../working_data/users_with_at_least_one_tweet_in_zoneA.json'))\n",
    "len(users_with_one_tweet_in_zoneA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_out_simplified_geo_contextual(tuple_of_uid_tweets):\n",
    "    if tuple_of_uid_tweets[0] in users_with_one_tweet_in_zoneA:\n",
    "        u_tweets = list(tuple_of_uid_tweets[1])\n",
    "        u_tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "        fileName = u_tweets[0]['actor']['preferredUsername'].lower()\n",
    "        write_full_tweets_to_geojsonl('/data/chime/geo/users_with_a_tweet_in_zone_a/'+fileName,u_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterate through the grouped by user tweets and if the uid matches a user with a tweet in zoneA, then write it out!\n",
    "tweets_gb_user.foreach(write_out_simplified_geo_contextual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.2)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
