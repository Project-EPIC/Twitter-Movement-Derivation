{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, iso8601, pprint, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Clean, Sort, Export from GNIP (Full Contextual Streams)\n",
    "\n",
    "This notebook: \n",
    "\n",
    "1. Loads raw gnip data\n",
    "1. Writes full GNIP GEOJSONL files per user\n",
    "\n",
    "## [Spark Status](http://epic-analytics.cs.colorado.edu:4040/jobs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = \"/data/chime/geo/sandy_new_jersey_geovulnerable_contextual/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the files\n",
    "raw_strings = sc.textFile(\"/data/chime/sandy/movement-derivation/sandy_new_jersey_geovulnerable_GNIP_RAW/*\")\n",
    "\n",
    "#Filter out the duds\n",
    "strings = raw_strings.filter(lambda x: x!=\"\")\n",
    "\n",
    "#JSONs\n",
    "jsons  = strings.map(json.loads)\n",
    "\n",
    "tweet_jsons = jsons.filter(lambda x: 'info' not in x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load all the Tweets!\n",
    "### Crucial Step:\n",
    "1. The GNIP `geo` field is backwards from convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_geo(t):\n",
    "    if 'geo' in t:\n",
    "        t['geo']['coordinates'].reverse()\n",
    "    return t\n",
    "tweets = tweet_jsons.map(fix_geo)\n",
    "tweets.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that this is working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'actor': {u'displayName': u'@the_goose_609',\n",
      "            u'favoritesCount': 91,\n",
      "            u'followersCount': 285,\n",
      "            u'friendsCount': 258,\n",
      "            u'id': u'id:twitter.com:401440617',\n",
      "            u'image': u'https://si0.twimg.com/profile_images/2838160619/309b6f0f60f275c818ff1002a0e36b01_normal.jpeg',\n",
      "            u'languages': [u'en'],\n",
      "            u'link': u'http://www.twitter.com/The_Goose_609',\n",
      "            u'links': [{u'href': None, u'rel': u'me'}],\n",
      "            u'listedCount': 0,\n",
      "            u'location': {u'displayName': u'da Trap 609',\n",
      "                          u'objectType': u'place'},\n",
      "            u'objectType': u'person',\n",
      "            u'postedTime': u'2011-10-30T15:08:19.000Z',\n",
      "            u'preferredUsername': u'The_Goose_609',\n",
      "            u'statusesCount': 6553,\n",
      "            u'summary': u'Shoot first...ask questions last #TeamFollowRealMotherFuckers #TeamFollowBackRealMotherFuckers #DMB',\n",
      "            u'twitterTimeZone': u'Central Time (US & Canada)',\n",
      "            u'utcOffset': u'-21600',\n",
      "            u'verified': False},\n",
      " u'body': u'Cash rules',\n",
      " u'generator': {u'displayName': u'Twitter for iPhone',\n",
      "                u'link': u'http://twitter.com/download/iphone'},\n",
      " u'gnip': {u'matching_rules': [{u'id': 129999400729396293, u'tag': None}]},\n",
      " u'id': u'tag:search.twitter.com,2005:270879450219048962',\n",
      " u'link': u'http://twitter.com/The_Goose_609/statuses/270879450219048962',\n",
      " u'object': {u'id': u'object:search.twitter.com,2005:270879450219048962',\n",
      "             u'link': u'http://twitter.com/The_Goose_609/statuses/270879450219048962',\n",
      "             u'objectType': u'note',\n",
      "             u'postedTime': u'2012-11-20T13:21:08.000Z',\n",
      "             u'summary': u'Cash rules'},\n",
      " u'objectType': u'activity',\n",
      " u'postedTime': u'2012-11-20T13:21:08.000Z',\n",
      " u'provider': {u'displayName': u'Twitter',\n",
      "               u'link': u'http://www.twitter.com',\n",
      "               u'objectType': u'service'},\n",
      " u'retweetCount': 0,\n",
      " u'twitter_entities': {u'hashtags': [], u'urls': [], u'user_mentions': []},\n",
      " u'verb': u'post'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tweets.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Group tweets by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[19] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_gb_user = tweets.groupBy(lambda t: t['actor']['id'])\n",
    "tweets_gb_user.cache() #We should probably cache these? If we want to use them again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on the status of this operation, should see a tuple of: `(user_id, iterable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'id:twitter.com:100233699',\n",
      " <pyspark.resultiterable.ResultIterable object at 0x7f2152bd9d90>)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tweets_gb_user.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Export Functions (Requires Local Arrays)\n",
    "(Also ALWAYS sorts by time; this could be expensive for large collections, but it's important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_full_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "        for t in tweets:\n",
    "            if 'geo' in t:\n",
    "                geo = t['geo']\n",
    "            else:\n",
    "                geo = None\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':geo,\n",
    "                'properties':t #Full GNIP Tweet in the properties\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_simplified_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "        for t in tweets:\n",
    "            if 'location' in t:\n",
    "                loc = t['location']\n",
    "            else:\n",
    "                loc = None\n",
    "            if 'location' in t['actor']:\n",
    "                u_loc = t['actor']['location']\n",
    "            else:\n",
    "                u_loc = None\n",
    "            if 'geo' in t:\n",
    "                geo = t['geo']\n",
    "            else:\n",
    "                geo = None\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':geo,\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'uid' :t['actor']['id'],\n",
    "                    'u_loc':u_loc,\n",
    "                    'u_reg':t['actor']['postedTime'],\n",
    "                    'u_sum':t['actor']['summary'],\n",
    "                    'tid' :t['id'],\n",
    "                    'loc' :loc,\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body'],\n",
    "                    'source':t['generator'],\n",
    "                    'verb':t['verb'],\n",
    "                    'meta':t['twitter_entities'],\n",
    "                    'u_utc':t['actor']['utcOffset']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_bare_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        for t in tweets:\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':t['geo'],\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write out the `geojsonl` files\n",
    "\n",
    "For now just writing the full GNIP files, but in the future, this can probably be streamlined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_out_full_contextual(tuple_of_uid_tweets):\n",
    "    u_tweets = list(tuple_of_uid_tweets[1])\n",
    "    u_tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "    fileName = u_tweets[0]['actor']['preferredUsername'].lower()\n",
    "    write_full_tweets_to_geojsonl(output_directory+'/'+fileName,u_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_gb_user.foreach( write_out_full_contextual )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.2)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
