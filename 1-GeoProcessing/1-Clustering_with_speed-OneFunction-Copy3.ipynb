{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBScan Clustering\n",
    "\n",
    "1. Using DBScan, find spatial clusters relevant to each user, typically home, work, or school.\n",
    "1. Also identify the speeds between each geo-tagged tweet.\n",
    "\n",
    "**Input**: Directory of Twitterers (in `geojsonl` format)<br>\n",
    "**Output**: Directory of Twitterers in a GeoPandas GeoDataFrame (written to JSON)\n",
    "\n",
    "### This notebook runs the scripts in ONE worker function, if it hangs, there are still results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_directory  = \"/data/chime/geo/matthew/brevard_zone_a_users/\"\n",
    "# output_directory = \"/data/chime/geo/matthew/brevard_zone_a_clustered_with_speed_gdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_directory  = \"/data/chime/geo2/NJ/AtlanticCity/\"\n",
    "output_directory = \"/data/chime/geo2/PROCESSED/NJ/AtlanticCity_Stage1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be safe to \"run all\" if the above directories are set :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json, matplotlib, iso8601, sys, time, datetime, pytz\n",
    "import numpy as np; import pandas as pd; import geopandas as gpd\n",
    "from shapely.geometry import shape, mapping, MultiPoint\n",
    "from multiprocessing import Pool, Manager;\n",
    "from dbscan_python import dbscan\n",
    "from geodistance import geodistance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1697 users in /data/chime/geo2/NJ/AtlanticCity/\n"
     ]
    }
   ],
   "source": [
    "users_in = sorted(os.listdir(input_directory))\n",
    "print(\"Found {0} users in {1}\".format(len(users_in), input_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # #sample?\n",
    "# users_in = np.random.choice(users_in, 100, replace=False)\n",
    "# len(users_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major Worker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def worker_function(args):\n",
    "    \"\"\"\n",
    "    Input: User .geojsonl file\n",
    "    Returns: DataFrame with speed & clusters\n",
    "\n",
    "    #Process\n",
    "    1. Loads all of a user's tweets (parseable json per line, parse, strip down to details, sort by time)\n",
    "    2. Computes Clusters based on below EPS & MIN_PTS\n",
    "    3. Computes time, distance, and speed between each tweet\n",
    "    4. Puts it all into a DataFrame\n",
    "    5. Finds Cluster Centroids\n",
    "    6. Writes files to disk\n",
    "\n",
    "    \"\"\"\n",
    "    #Unpack the arguments\n",
    "    user_geojsonl_file, input_directory, output_directory, q = args\n",
    "\n",
    "    EPS     = 200     #Max. Distance for points in the cluster... (In meters)                                                                                                                  \n",
    "    MIN_PTS = 5       #Minimum Points in a cluster...\n",
    "    \n",
    "    geo_tweets     = []\n",
    "    non_geo_tweets = []\n",
    "    for line in open(input_directory+\"/\"+user_geojsonl_file,'r'):\n",
    "        t = json.loads(line.strip())\n",
    "        if t['geometry']:\n",
    "            stripped = {\n",
    "                    'geometry': shape(t['geometry']),\n",
    "                    'coords'  : t['geometry']['coordinates'],\n",
    "                    'date'    : pd.Timestamp(t['properties']['postedTime']),\n",
    "                    'text'    : t['properties']['body'],\n",
    "                    'user'    : t['properties']['actor']['preferredUsername'],\n",
    "                    'uid'     : t['properties']['actor']['id'].split(\":\")[2],\n",
    "                    'tweet_id': t['properties']['id'].split(\":\")[2]\n",
    "                }\n",
    "            geo_tweets.append(stripped)\n",
    "        else:\n",
    "            t = {\n",
    "                    'date'    : pd.Timestamp(t['properties']['postedTime']),\n",
    "                    'text'    : t['properties']['body'],\n",
    "                    'user'    : t['properties']['actor']['preferredUsername'],\n",
    "                    'uid'     : t['properties']['actor']['id'].split(\":\")[2],\n",
    "                    'tweet_id': t['properties']['id'].split(\":\")[2]\n",
    "                }\n",
    "            non_geo_tweets.append(t)\n",
    "\n",
    "    #Exit case 1: There aren't enough points\n",
    "    if len(geo_tweets)<MIN_PTS:\n",
    "        q.put(1)\n",
    "        return None\n",
    "\n",
    "    #Ensure we're sorted by time (Safety measure, probably taking performance hit)\n",
    "    geo_tweets.sort(key=lambda t: t['date'])\n",
    "\n",
    "    #Clustering:\n",
    "    points = [t['coords'] for t in geo_tweets]\n",
    "    clusters = dbscan.dbscan(np.matrix([ [p[0] for p in points], [p[1] for p in points] ]), EPS, MIN_PTS)\n",
    "    \n",
    "    #Iterate through tweets & clusters to assign cluster & calcualte distances\n",
    "    for idx, t in enumerate(geo_tweets):\n",
    "        t['cluster'] = clusters[idx]\n",
    "        if idx>0:\n",
    "            p1 = t['geometry']\n",
    "            p2 = geo_tweets[idx-1]['geometry']\n",
    "            t['geo_delta'] = geodistance.distanceHaversine(p1.y,p1.x,p2.y,p2.x)[0]*1000\n",
    "    \n",
    "    #Now we can put all the tweets into a DataFrame!\n",
    "    \n",
    "    df = gpd.GeoDataFrame(geo_tweets+non_geo_tweets)\n",
    "\n",
    "    # If clusters were all -1, then return nothing, it couldn't cluster!\n",
    "    if not len(df.query('cluster>=0'))>=1:\n",
    "        q.put(2)\n",
    "        return None\n",
    "    df['time_delta'] = df['date'].diff()\n",
    "    df['speed'] = df.apply(lambda row: row['geo_delta']/ (row['time_delta'] / np.timedelta64(1, 's')), axis=1)\n",
    "    df = df.sort_values(by='date').reset_index(drop=True)\n",
    "    \n",
    "    #Calculate Cluster Centroids\n",
    "    centroids = df.groupby('cluster', as_index=False).aggregate({'geometry':lambda x: MultiPoint(list(x)).centroid})\n",
    "    centroids.rename(columns={'geometry' : 'cluster_center'}, inplace=True)\n",
    "    centroids.set_index(centroids.cluster, inplace=True)\n",
    "    centroids.set_value(-1,'cluster_center',None)\n",
    "   \n",
    "    df = df.merge(centroids, on='cluster', how='left')\n",
    "    \n",
    "    #Now write it out\n",
    "    df['date'] = df['date'].apply(lambda t: datetime.datetime.strftime(t,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "    df['time_delta'] = df['time_delta'] / np.timedelta64(1, 's')\n",
    "    df['cluster_center'] = df['cluster_center'].apply(lambda c: json.dumps(safe_mapping(c)))\n",
    "    uName = df.head(1).user.values[0].lower() # Grab username, always make it lowercase for sorting safety :) \n",
    "    \n",
    "    clean = df.where((pd.notnull(df)), None)\n",
    "    geojson = {\"type\":\"FeatureCollection\",\"features\":[]}\n",
    "    for _, row in clean.iterrows():\n",
    "        geom = safe_mapping(row.geometry)\n",
    "        feature = {'type':'Feature',\n",
    "                   'geometry':geom,\n",
    "                   'properties':row.to_dict()\n",
    "                    }\n",
    "        del feature['properties']['geometry']\n",
    "        geojson['features'].append(feature)\n",
    "    \n",
    "    with open(output_directory+\"/\"+uName+'.geojson','w') as oFile:\n",
    "        json.dump(geojson, oFile) \n",
    "\n",
    "    if q is not None:\n",
    "        q.put(1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_mapping(p):\n",
    "    if p==None or np.isnan(p).any():\n",
    "        return None\n",
    "    else:\n",
    "        return mapping(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the super function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1697 users...\n",
      "Processed: 1536, 90.5%"
     ]
    }
   ],
   "source": [
    "# Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i,input_directory,output_directory,q) for i in users_in]\n",
    "result = p.map_async(worker_function, args)\n",
    "\n",
    "sys.stderr.write(\"Processing {0} users...\\n\".format(len(users_in)))\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(1)\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))\n",
    "\n",
    "values = result.get()\n",
    "users = [i for i in values if i is not None]\n",
    "nones = [i for i in values if i is None]\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
