{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2, multiprocessing, psycopg2.extras, os, json, sys, time, scipy, datetime\n",
    "from multiprocessing import Pool, Manager \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_directory  = \"/data/chime/geo/sandy_zone_a/clustered_with_speed_gdf\"\n",
    "output_directory = \"/data/chime/geo/sandy_zone_a/stage_2_temporal_clusters_with_home\"\n",
    "\n",
    "_landfall_str = '201210300000'\n",
    "_start_str    = '201210290000'\n",
    "_end_str      = '201210310000'\n",
    "_landfall = pd.Timestamp(_landfall_str)\n",
    "_start    = pd.Timestamp(_start_str)\n",
    "_end      = pd.Timestamp(_end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load all the geo-clustered tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4022 users in /data/chime/geo/sandy_zone_a/clustered_with_speed_gdf\n"
     ]
    }
   ],
   "source": [
    "users_in = sorted(os.listdir(input_directory))\n",
    "print(\"Found {0} users in {1}\".format(len(users_in), input_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loader_function(args):\n",
    "    uFile, path, q = args\n",
    "    u = json.load(open(path+\"/\"+uFile,'r'))\n",
    "    tweets = []\n",
    "    for t in u['features']:\n",
    "        t['properties']['geometry'] = shape(t['geometry'])\n",
    "        t['properties']['date'] = pd.Timestamp(t['properties']['date'])\n",
    "        tweets.append(t['properties'])\n",
    "    q.put(1)\n",
    "    return gpd.GeoDataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 200, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, input_directory, q) for i in users_in[:200]]\n",
    "result = p.map_async(loader_function, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))\n",
    "users = result.get()\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4249 total tweets for all users\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} total tweets for all users\".format(sum([len(x) for x in users])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Identifying Temporal Clusters\n",
    "Use a custom _worker_ function to find specific time clusters\n",
    "\n",
    "## 1. Enough Tweets?\n",
    "\n",
    "Ensure that we have the following for each user:\n",
    "1. Geo-Cluter Information (If no geo-clusters are available, remove)\n",
    "2. Enough Tweets (At least *A* Tweet during the storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_cluster(t):\n",
    "    t = t.tz_convert(\"EST\")\n",
    "    '''Get the timecluster'''\n",
    "    hour = t.hour//4 + 1\n",
    "    if t.weekday()>4:\n",
    "        return 6+hour\n",
    "    else:\n",
    "        return hour\n",
    "    \n",
    "def worker_function(args):\n",
    "    userDF, q = args\n",
    "    \n",
    "    #If no tweets around the time of the storm, then fail.\n",
    "    if len(userDF.query(\"date > %s & date < %s\"%(_start_str, _end_str))) < 1:\n",
    "        q.put(1)\n",
    "        return None\n",
    "    \n",
    "    userDF['day_cluster'] = userDF.date.apply(lambda t: time_cluster(t))\n",
    "    q.put(0)\n",
    "    return userDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 200, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, q) for i in users]\n",
    "result = p.map_async(worker_function, args)\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))\n",
    "\n",
    "values = result.get()\n",
    "x = [i for i in values  if i is not None]\n",
    "nones = [i for i in values  if i is None]\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 15 users\n",
      "185 Users failed\n"
     ]
    }
   ],
   "source": [
    "print(\"Successfully processed {0} users\\n{1} Users failed\".format(len(x),len(nones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_users = sorted(x, key=lambda y: len(y), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Group the clusters and find which geo-clusters correspond with home hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_clusters(df):\n",
    "    \"\"\"\n",
    "    There is definitely room for the logic in _this_ function to improve, but for now it looks good :) \n",
    "    \"\"\"\n",
    "    gb_geo = df.query('date < '+_start_str).groupby('cluster')\n",
    "    if len(gb_geo) < 1:\n",
    "        return (None,None)\n",
    "    _agged = gb_geo['day_cluster'].agg({\"tweets\":pd.Series.count,\n",
    "                                        \"Number Unique Times\":pd.Series.nunique,\n",
    "                                        \"day_cluster_counts\": lambda t: Counter(t),\n",
    "                                        \"HomeTimes\": lambda t: any(t==1) or any(t==2) or any(t==6),\n",
    "                                      }).sort_values('Number Unique Times', ascending=False)\n",
    "    hc = None\n",
    "#   If the highest rated cluster (unique timewise includes HomeTimes, then return that)\n",
    "    if(_agged.HomeTimes.values[0]):\n",
    "        hc = _agged.iloc[0].name\n",
    "    return hc, _agged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1 of 15\r",
      "2 of 15\r",
      "3 of 15\r",
      "4 of 15\r",
      "5 of 15\r",
      "6 of 15\r",
      "7 of 15\r",
      "8 of 15\r",
      "9 of 15\r",
      "10 of 15\r",
      "11 of 15\r",
      "12 of 15\r",
      "13 of 15\r",
      "14 of 15\r",
      "15 of 15\n",
      "\n",
      "Clustered: 3, Failed: 12"
     ]
    }
   ],
   "source": [
    "vals= []\n",
    "len_users = len(_users)\n",
    "user_collection = []\n",
    "\n",
    "user_meta_collection = []\n",
    "for idx, U in enumerate(_users):\n",
    "    \n",
    "    hc = rank_clusters(U)[0]\n",
    "    if hc is not None and hc < 0:\n",
    "        hc = None\n",
    "        \n",
    "    user_meta_collection.append({\n",
    "            'user':U['user'].values[0],\n",
    "            'uid' :U['uid'].values[0],\n",
    "            'tweets':len(U),\n",
    "            'home_cluster': hc\n",
    "        })\n",
    "\n",
    "    U['home_cluster_id'] = pd.Series(hc)\n",
    "    \n",
    "    sys.stderr.write(\"\\r{0} of {1}\".format(idx+1, len_users))\n",
    "_user_meta = pd.DataFrame(user_meta_collection)\n",
    "sys.stderr.write(\"\\n\\nClustered: {0}, Failed: {1}\".format( len(_user_meta[~np.isnan(_user_meta.home_cluster)]),\n",
    "                                                           len(_user_meta[np.isnan(_user_meta.home_cluster)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_with_homes = [u for u in _users if u.home_cluster_id.count()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export the User Meta Dataframe first\n",
    "with open(output_directory+'/'+'temporal_clustered_user_meta.json','w') as metaOut:\n",
    "    metaOut.write(_user_meta.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Write these users to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_json_export(args):\n",
    "    df, path, q = args\n",
    "    df = df.copy()\n",
    "    df['date'] = df['date'].apply(lambda t: datetime.datetime.strftime(t,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "    uName = df.head(1).user.values[0].lower()\n",
    "    with open(path+\"/\"+uName+'.geojson','w') as oFile:\n",
    "        oFile.write(df.to_json(ensure_ascii=False))\n",
    "    if q is not None:\n",
    "        q.put(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting 3 users to /data/chime/geo/sandy_zone_a/stage_2_temporal_clusters_with_home\n",
      "Processed: 3, 100%"
     ]
    }
   ],
   "source": [
    "#Parallel runtime\n",
    "p = Pool(30)\n",
    "m = Manager()\n",
    "q = m.Queue()\n",
    "\n",
    "args = [(i, output_directory, q) for i in users_with_homes]\n",
    "result = p.map_async(safe_json_export, args)\n",
    "\n",
    "sys.stderr.write(\"Exporting {0} users to {1}\\n\".format(len(args),output_directory))\n",
    "\n",
    "# monitor loop\n",
    "while True:\n",
    "    if result.ready():\n",
    "        break\n",
    "    else:\n",
    "        size = q.qsize()\n",
    "        sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(size, size/len(args)*100))\n",
    "        time.sleep(0.5)\n",
    "p.close()\n",
    "sys.stderr.write(\"\\rProcessed: {0}, {1:.3g}%\".format(q.qsize(), q.qsize()/len(args)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "# End of Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "# Beginning of Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
