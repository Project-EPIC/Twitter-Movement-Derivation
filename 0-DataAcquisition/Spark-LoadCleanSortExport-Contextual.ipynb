{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, iso8601, pprint, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Clean, Sort, Export from GNIP (Full Contextual Streams)\n",
    "\n",
    "This notebook: \n",
    "\n",
    "1. Loads raw gnip data\n",
    "1. Writes full GNIP GEOJSONL files per user\n",
    "\n",
    "## [Spark Status](http://epic-analytics.cs.colorado.edu:4040/jobs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = \"/data/chime/geo/zone_a_users_full_contextual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the files\n",
    "raw_strings = sc.textFile(\"/data/chime/sandy/movement-derivation/ZoneA_Users_with_HL_GNIP/*\")\n",
    "\n",
    "#Filter out the duds\n",
    "strings = raw_strings.filter(lambda x: x!=\"\")\n",
    "\n",
    "#JSONs\n",
    "jsons  = strings.map(json.loads)\n",
    "\n",
    "tweet_jsons = jsons.filter(lambda x: 'info' not in x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load all the Tweets!\n",
    "### Crucial Step:\n",
    "1. The GNIP `geo` field is backwards from convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[30] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_geo(t):\n",
    "    if 'geo' in t:\n",
    "        t['geo']['coordinates'].reverse()\n",
    "    return t\n",
    "tweets = tweet_jsons.map(fix_geo)\n",
    "tweets.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that this is working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'actor': {u'displayName': u'PROF | tajuAhmed |',\n",
      "            u'favoritesCount': 33,\n",
      "            u'followersCount': 181,\n",
      "            u'friendsCount': 149,\n",
      "            u'id': u'id:twitter.com:177665053',\n",
      "            u'image': u'https://si0.twimg.com/profile_images/2581374515/image_normal.jpg',\n",
      "            u'languages': [u'en'],\n",
      "            u'link': u'http://www.twitter.com/mutalib_ahmed',\n",
      "            u'links': [{u'href': None, u'rel': u'me'}],\n",
      "            u'listedCount': 0,\n",
      "            u'location': {u'displayName': u'Bronx - Newyork',\n",
      "                          u'objectType': u'place'},\n",
      "            u'objectType': u'person',\n",
      "            u'postedTime': u'2010-08-12T19:09:07.000Z',\n",
      "            u'preferredUsername': u'mutalib_ahmed',\n",
      "            u'statusesCount': 9742,\n",
      "            u'summary': u'',\n",
      "            u'twitterTimeZone': u'Quito',\n",
      "            u'utcOffset': u'-18000',\n",
      "            u'verified': False},\n",
      " u'body': u\"@pslymaabanah and broke ass niqqas' smfh\",\n",
      " u'generator': {u'displayName': u'Twitter for iPhone',\n",
      "                u'link': u'http://twitter.com/download/iphone'},\n",
      " u'gnip': {u'matching_rules': [{u'id': 8438911990214585875, u'tag': None}]},\n",
      " u'id': u'tag:search.twitter.com,2005:248547057743298560',\n",
      " u'inReplyTo': {u'link': u'http://twitter.com/pslymaabanah/statuses/248546520629137408'},\n",
      " u'link': u'http://twitter.com/mutalib_ahmed/statuses/248547057743298560',\n",
      " u'object': {u'id': u'object:search.twitter.com,2005:248547057743298560',\n",
      "             u'link': u'http://twitter.com/mutalib_ahmed/statuses/248547057743298560',\n",
      "             u'objectType': u'note',\n",
      "             u'postedTime': u'2012-09-19T22:20:11.000Z',\n",
      "             u'summary': u\"@pslymaabanah and broke ass niqqas' smfh\"},\n",
      " u'objectType': u'activity',\n",
      " u'postedTime': u'2012-09-19T22:20:11.000Z',\n",
      " u'provider': {u'displayName': u'Twitter',\n",
      "               u'link': u'http://www.twitter.com',\n",
      "               u'objectType': u'service'},\n",
      " u'retweetCount': 0,\n",
      " u'twitter_entities': {u'hashtags': [],\n",
      "                       u'urls': [],\n",
      "                       u'user_mentions': [{u'id': 67471576,\n",
      "                                           u'id_str': u'67471576',\n",
      "                                           u'indices': [0, 13],\n",
      "                                           u'name': u'cockymahfucker',\n",
      "                                           u'screen_name': u'pslymaabanah'}]},\n",
      " u'verb': u'post'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tweets.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Group tweets by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[36] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_gb_user = tweets.groupBy(lambda t: t['actor']['id'])\n",
    "tweets_gb_user.cache() #We should probably cache these? If we want to use them again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on the status of this operation, should see a tuple of: `(user_id, iterable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'id:twitter.com:848470111',\n",
      " <pyspark.resultiterable.ResultIterable object at 0x7f5ce2f76d90>)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tweets_gb_user.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Export Functions (Requires Local Arrays)\n",
    "(Also ALWAYS sorts by time; this could be expensive for large collections, but it's important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_full_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "        for t in tweets:\n",
    "            if 'geo' in t:\n",
    "                geo = t['geo']\n",
    "            else:\n",
    "                geo = None\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':geo,\n",
    "                'properties':t #Full GNIP Tweet in the properties\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_simplified_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "        for t in tweets:\n",
    "            if 'location' in t:\n",
    "                loc = t['location']\n",
    "            else:\n",
    "                loc = None\n",
    "            if 'location' in t['actor']:\n",
    "                u_loc = t['actor']['location']\n",
    "            else:\n",
    "                u_loc = None\n",
    "            if 'geo' in t:\n",
    "                geo = t['geo']\n",
    "            else:\n",
    "                geo = None\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':geo,\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'uid' :t['actor']['id'],\n",
    "                    'u_loc':u_loc,\n",
    "                    'u_reg':t['actor']['postedTime'],\n",
    "                    'u_sum':t['actor']['summary'],\n",
    "                    'tid' :t['id'],\n",
    "                    'loc' :loc,\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body'],\n",
    "                    'source':t['generator'],\n",
    "                    'verb':t['verb'],\n",
    "                    'meta':t['twitter_entities'],\n",
    "                    'u_utc':t['actor']['utcOffset']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_bare_tweets_to_geojsonl(fileName, tweets):\n",
    "    with open(fileName+'.geojsonl','w') as outFile:\n",
    "        for t in tweets:\n",
    "            geojson = {\n",
    "                'type':'Feature',\n",
    "                'geometry':t['geo'],\n",
    "                'properties':{\n",
    "                    'user':t['actor']['preferredUsername'],\n",
    "                    'time':t['postedTime'],\n",
    "                    'text':t['body']\n",
    "                }\n",
    "            }\n",
    "            outFile.write(json.dumps(geojson)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write out the `geojsonl` files\n",
    "\n",
    "For now just writing the full GNIP files, but in the future, this can probably be streamlined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_out_full_contextual(tuple_of_uid_tweets):\n",
    "    u_tweets = list(tuple_of_uid_tweets[1])\n",
    "    u_tweets.sort(key=lambda t: iso8601.parse_date(t['postedTime']))\n",
    "    fileName = u_tweets[0]['actor']['preferredUsername'].lower()\n",
    "    write_full_tweets_to_geojsonl(output_directory+'/'+fileName,u_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_gb_user.foreach( write_out_full_contextual )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.2)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
